{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# add project root folder to path to allow import local modules\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "# import local modules\n",
    "from helpers import compute_ridge_loss\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_Quantiles = 100\n",
    "Quantile_Transformer = False\n",
    "Box_Cox= False\n",
    "Logarithm = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make custom pipeline to create normal distributions for the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Transformation_Preprocessing(Preprocessing):\n",
    "    def __init__(self,dataset):\n",
    "        super().__init__(dataset)\n",
    "        self.n_quantiles = None\n",
    "        self.quantile_transformer = None\n",
    "        self.box_cox= None\n",
    "        self.logarithm = None\n",
    "    \n",
    "    def set_transformation_type(self,n_quantiles, quantile_transformer, box_cox, logarithm):\n",
    "        self.n_quantiles = n_quantiles\n",
    "        self.quantile_transformer = quantile_transformer\n",
    "        self.box_cox = box_cox\n",
    "        self.logarithm = logarithm\n",
    "        \n",
    "    def _feature_transformation(self):\n",
    "        super()._feature_transformation() \n",
    "        \n",
    "        dataset =self.dataset\n",
    "        \n",
    "        if self.logarithm:\n",
    "            for i in range(self.dataset.shape[1]):\n",
    "                #print('before : ', self.dataset[:,i])\n",
    "                self.dataset[:,i] = np.array([np.sign(x)*np.log(1+ abs(x)) for x in dataset[:,i]])             \n",
    "                #print('after:', self.dataset[:,i])\n",
    "        \n",
    "        if self.quantile_transformer:\n",
    "            for i in range(self.dataset.shape[1]):\n",
    "                qt = QuantileTransformer(self.n_quantiles, output_distribution= 'normal')\n",
    "                new_feature = qt.fit_transform(self.dataset[:,i].reshape(-1,1))\n",
    "                self.dataset[:,i] = new_feature.flatten()\n",
    "        \n",
    "        if self.box_cox:\n",
    "            for i in range(dataset.shape[1]):     \n",
    "                bc = PowerTransformer(method='box-cox')\n",
    "                new_feature = bc.fit_transform(dataset[:,i].reshape(-1,1))\n",
    "                self.dataset[:,i] = new_feature.flatten()\n",
    "           \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train = Feature_Transformation_Preprocessing(load_csv('../dataset/trainset.csv'))\n",
    "p_test = Feature_Transformation_Preprocessing(load_csv('../dataset/testset.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train.set_transformation_type(N_Quantiles, Quantile_Transformer, Box_Cox, Logarithm)\n",
    "p_test.set_transformation_type(N_Quantiles, Quantile_Transformer, Box_Cox, Logarithm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before :  [121.78731104  28.478       74.538      ...  62.328      133.204\n",
      " 121.78731104]\n",
      "after: [4.81045368 3.38364422 4.32463584 ... 4.14832757 4.89936103 4.81045368]\n",
      "before :  [ 90.1    52.063 119.834 ...  42.916  17.066  72.435]\n",
      "after: [4.5119578  3.97147989 4.7944177  ... 3.78227872 2.89403172 4.29640066]\n",
      "before :  [44.395 21.773 66.795 ... 44.607 54.913 59.817]\n",
      "after: [3.81540197 3.12557562 4.21648845 ... 3.82006121 4.02379691 4.10786936]\n",
      "before :  [  1.774  54.51   44.244 ...  68.272 151.277   0.161]\n",
      "after: [1.02029032 4.01656318 3.81207006 ... 4.23804078 5.02570123 0.1492817 ]\n",
      "before :  [2.40016552 2.40016552 0.658      ... 2.40016552 0.554      2.40016552]\n",
      "after: [1.22382411 1.22382411 0.50561206 ... 1.22382411 0.44083225 1.22382411]\n",
      "before :  [371.54454611 371.54454611  28.226      ... 371.54454611 144.171\n",
      " 371.54454611]\n",
      "after: [5.92035662 5.92035662 3.37505872 ... 5.92035662 4.97791236 5.92035662]\n",
      "before :  [-0.81284729 -0.81284729 -0.094      ... -0.81284729  0.087\n",
      " -0.81284729]\n",
      "after: [-0.5948987  -0.5948987  -0.0898407  ... -0.5948987   0.08342161\n",
      " -0.5948987 ]\n",
      "before :  [1.377 0.681 1.686 ... 1.454 1.734 1.844]\n",
      "after: [0.86583919 0.51938885 0.9880531  ... 0.89771935 1.00576574 1.04521151]\n",
      "before :  [ 1.774  4.461 26.519 ... 17.061 21.68   0.161]\n",
      "after: [1.02029032 1.69763192 3.31487668 ... 2.89375492 3.12148348 0.1492817 ]\n",
      "before :  [ 64.763 117.301 171.969 ... 116.991 277.735  68.864]\n",
      "after: [4.18605737 4.77323222 5.15311239 ... 4.77060835 5.63026151 4.2465505 ]\n",
      "before :  [1.37  1.68  3.882 ... 0.896 0.816 0.87 ]\n",
      "after: [0.86288996 0.98581679 1.58555497 ... 0.6397464  0.59663628 0.62593843]\n",
      "before :  [-1.395 -0.313 -1.4   ...  0.597  1.234 -1.411]\n",
      "after: [-0.87338323 -0.2723146  -0.87546874 ...  0.46812687  0.8037937\n",
      " -0.8800416 ]\n",
      "before :  [0.45814619 0.45814619 0.01       ... 0.45814619 0.         0.45814619]\n",
      "after: [0.3771659  0.3771659  0.00995033 ... 0.3771659  0.         0.3771659 ]\n",
      "before :  [27.323 25.083 20.657 ... 34.614 39.773 36.832]\n",
      "after: [3.3436742  3.26128376 3.07532873 ... 3.57273882 3.7080201  3.63315531]\n",
      "before :  [-0.574  1.784 -0.448 ... -1.071  0.432 -0.858]\n",
      "after: [-0.45362015  1.02388874 -0.37018329 ... -0.72803158  0.35907207\n",
      " -0.61950064]\n",
      "before :  [ 1.457 -1.218 -1.028 ... -1.491  0.994 -0.451]\n",
      "after: [ 0.89894109 -0.79660589 -0.70705009 ... -0.91268424  0.69014267\n",
      " -0.37225297]\n",
      "before :  [37.44  42.143 80.189 ... 31.03  32.471 32.032]\n",
      "after: [3.64909858 3.76452018 4.39677977 ... 3.46667296 3.51067939 3.49747679]\n",
      "before :  [-1.585  1.859  0.582 ... -0.563  0.579 -1.847]\n",
      "after: [-0.94972551  1.05047191  0.45868987 ... -0.44660705  0.45679174\n",
      " -1.04626581]\n",
      "before :  [ 0.521 -0.541 -2.363 ... -2.853 -0.734 -2.007]\n",
      "after: [ 0.41936801 -0.43243156 -1.21283343 ... -1.34885207 -0.55043088\n",
      " -1.1009429 ]\n",
      "before :  [ 56.246  17.969  48.42  ...  25.6   111.985  49.302]\n",
      "after: [4.04735777 2.94280607 3.9003552  ... 3.28091122 4.72725507 3.91804484]\n",
      "before :  [-2.237 -3.022  1.335 ... -1.123 -0.45   1.983]\n",
      "after: [-1.17464697 -1.39177929  0.84801189 ... -0.75283018 -0.37156356\n",
      "  1.09292951]\n",
      "before :  [ 93.023 138.79  250.953 ... 197.346 455.167  48.797]\n",
      "after: [4.54353943 4.9401413  5.52924256 ... 5.29001298 6.12285897 3.90795474]\n",
      "before :  [0. 1. 2. ... 1. 2. 0.]\n",
      "after: [0.         0.69314718 1.09861229 ... 0.69314718 1.09861229 0.        ]\n",
      "before :  [ 84.89923502  50.076       40.099      ...  51.348      142.539\n",
      "  84.89923502]\n",
      "after: [4.45317492 3.93331472 3.71598379 ... 3.95791373 4.96660678 4.45317492]\n",
      "before :  [-0.00590535  1.173       0.209      ... -2.577      -0.682\n",
      " -0.00590535]\n",
      "after: [-0.00588798  0.7761087   0.18979357 ... -1.27452446 -0.51998356\n",
      " -0.00588798]\n",
      "before :  [-0.01128315  2.079       0.83       ...  1.372       2.618\n",
      " -0.01128315]\n",
      "after: [-0.01121997  1.12460487  0.60431597 ...  0.86373348  1.28592139\n",
      " -0.01121997]\n",
      "before :  [57.70310934 57.70310934 31.023      ... 57.70310934 62.952\n",
      " 57.70310934]\n",
      "after: [4.0724927  4.0724927  3.46645439 ... 4.0724927  4.1581328  4.0724927 ]\n",
      "before :  [-0.01115017 -0.01115017 -0.449      ... -0.01115017 -0.128\n",
      " -0.01115017]\n",
      "after: [-0.01108846 -0.01108846 -0.37087366 ... -0.01108846 -0.12044615\n",
      " -0.01108846]\n",
      "before :  [-0.00314899 -0.00314899  0.57       ... -0.00314899 -2.185\n",
      " -0.00314899]\n",
      "after: [-0.00314404 -0.00314404  0.45107562 ... -0.00314404 -1.15845229\n",
      " -0.00314404]\n",
      "before :  [  0.     50.076  71.123 ...  51.348 205.491   0.   ]\n",
      "after: [0.         3.93331472 4.27837299 ... 3.95791373 5.33025683 0.        ]\n",
      "before :  [289.836 127.206  80.875 ...  90.285 123.209  80.902]\n",
      "after: [5.67275953 4.85363835 4.40519369 ... 4.51398648 4.82196563 4.40552341]\n",
      "before :  [18.352 11.315 42.875 ... 37.123  1.691 35.251]\n",
      "after: [2.96279577 2.51081803 3.78134468 ... 3.64081777 0.98991287 3.59046697]\n",
      "before :  [223.22   59.943  64.871 ...  63.387  70.586  50.432]\n",
      "after: [5.41262771 4.109939   4.18769828 ... 4.16491175 4.27089952 3.94026055]\n",
      "before :  [109.112 112.092 112.515 ...  19.58  136.681  65.685]\n",
      "after: [4.70149803 4.72820165 4.73193499 ... 3.02431973 4.92493942 4.19998004]\n",
      "before :  [2.041      2.43638635 2.43638635 ... 2.43638635 2.43638635 2.43638635]\n",
      "after: [1.11218641 1.23442044 1.23442044 ... 1.23442044 1.23442044 1.23442044]\n",
      "before :  [104.693     373.9678653 373.9678653 ... 373.9678653 373.9678653\n",
      " 373.9678653]\n",
      "after: [4.66053867 5.92684033 5.92684033 ... 5.92684033 5.92684033 5.92684033]\n",
      "before :  [-1.026      -0.90255826 -0.90255826 ... -0.90255826 -0.90255826\n",
      " -0.90255826]\n",
      "after: [-0.70606341 -0.64319943 -0.64319943 ... -0.64319943 -0.64319943\n",
      " -0.64319943]\n",
      "before :  [2.781 2.24  1.397 ... 2.921 1.545 2.063]\n",
      "after: [1.32998852 1.17557333 0.87421796 ... 1.36634672 0.93413065 1.11939483]\n",
      "before :  [23.406 59.069  3.912 ... 19.58   7.974  1.893]\n",
      "after: [3.194829   4.0954939  1.59168119 ... 3.02431973 2.19433151 1.06229403]\n",
      "before :  [240.424 121.107 212.335 ...  62.77  236.543 119.789]\n",
      "after: [5.48655472 4.80489771 5.3628637  ... 4.15528286 5.47034866 4.79404522]\n",
      "before :  [0.631 1.155 3.482 ... 0.826 0.486 1.069]\n",
      "after: [0.48919332 0.76779072 1.50006938 ... 0.60212778 0.39608795 0.7270654 ]\n",
      "before :  [ 1.399  0.69   0.065 ... -0.344  0.953  1.386]\n",
      "after: [ 0.87505198  0.52472853  0.0629748  ... -0.29565024  0.66936665\n",
      "  0.86961832]\n",
      "before :  [0.158      0.45960344 0.45960344 ... 0.45960344 0.45960344 0.45960344]\n",
      "after: [0.14669438 0.37816478 0.37816478 ... 0.37816478 0.37816478 0.37816478]\n",
      "before :  [94.538 30.324 23.011 ... 34.383 64.193 26.501]\n",
      "after: [4.55952407 3.44438458 3.17851206 ... 3.56623148 4.1773521  3.31422237]\n",
      "before :  [-0.598 -0.573 -0.178 ...  0.389 -0.884  0.008]\n",
      "after: [-0.46875285 -0.45298462 -0.16381809 ...  0.32858406 -0.63339718\n",
      "  0.00796817]\n",
      "before :  [-0.307 -1.229  2.07  ... -2.212  2.742 -0.224]\n",
      "after: [-0.26773443 -0.80155305  1.12167756 ... -1.1668938   1.31962023\n",
      " -0.20212418]\n",
      "before :  [59.695 35.038 80.13  ... 28.388 31.228 28.337]\n",
      "after: [4.10586132 3.58457394 4.39605281 ... 3.38058643 3.47283564 3.37884952]\n",
      "before :  [ 1.509 -1.034 -1.564 ... -0.03   0.305 -0.851]\n",
      "after: [ 0.91988427 -0.7100043  -0.94156854 ... -0.0295588   0.26620304\n",
      " -0.61572603]\n",
      "before :  [-2.121  2.862  1.902 ...  0.679 -2.555 -2.099]\n",
      "after: [-1.13815346  1.35118518  1.06540015 ...  0.51819838 -1.26835506\n",
      " -1.13107948]\n",
      "before :  [11.306 96.366 19.121 ... 12.163 60.121 34.024]\n",
      "after: [2.51008695 4.57847707 3.00176405 ... 2.57740986 4.11285551 3.55603354]\n",
      "before :  [-1.399  2.667  3.061 ... -2.369 -2.516 -0.892]\n",
      "after: [-0.87505198  1.29937389  1.40142925 ... -1.21461596 -1.25732398\n",
      " -0.63763447]\n",
      "before :  [330.716 222.183 176.33  ... 138.298 311.916 208.451]\n",
      "after: [5.80427918 5.40799206 5.1780124  ... 4.93661552 5.74593478 5.34448982]\n",
      "before :  [2. 1. 1. ... 0. 1. 1.]\n",
      "after: [1.09861229 0.69314718 0.69314718 ... 0.         0.69314718 0.69314718]\n",
      "before :  [ 43.467       55.745      109.195      ...  84.12344867 141.122\n",
      "  64.951     ]\n",
      "after: [3.79474734 4.03856755 4.70225152 ... 4.44410254 4.95668584 4.18891204]\n",
      "before :  [-0.897      -2.422       1.05       ...  0.02055515  0.81\n",
      " -0.111     ]\n",
      "after: [-0.64027369 -1.23022518  0.71783979 ...  0.02034675  0.59332685\n",
      " -0.10526051]\n",
      "before :  [ 2.235      -0.451      -1.067      ... -0.02244438  0.25\n",
      "  2.069     ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after: [ 1.17402893 -0.37225297 -0.72609828 ... -0.02219621  0.22314355\n",
      "  1.12135178]\n",
      "before :  [42.724      57.46327892 57.46327892 ... 57.46327892 57.46327892\n",
      " 57.46327892]\n",
      "after: [3.77789715 4.06839885 4.06839885 ... 4.06839885 4.06839885 4.06839885]\n",
      "before :  [ 1.144      -0.01820352 -0.01820352 ... -0.01820352 -0.01820352\n",
      " -0.01820352]\n",
      "after: [ 0.76267324 -0.01803982 -0.01803982 ... -0.01803982 -0.01803982\n",
      " -0.01803982]\n",
      "before :  [2.183      0.01274878 0.01274878 ... 0.01274878 0.01274878 0.01274878]\n",
      "after: [1.15782415 0.01266819 0.01266819 ... 0.01266819 0.01266819 0.01266819]\n",
      "before :  [ 86.191  55.745 109.195 ...   0.    141.122  64.951]\n",
      "after: [4.46810111 4.03856755 4.70225152 ... 0.         4.95668584 4.18891204]\n"
     ]
    }
   ],
   "source": [
    "y_train , x_train= p_train.preprocess()\n",
    "y_test, x_test = p_test.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.09260008  1.11915975 ...         nan         nan\n",
      "  -1.04963898]\n",
      " [ 1.         -2.71984142  0.2753565  ...         nan         nan\n",
      "   0.14948058]\n",
      " [ 1.         -1.09282203  1.6583506  ...         nan         nan\n",
      "   0.3794284 ]\n",
      " ...\n",
      " [ 1.         -1.45736772  0.0290911  ...         nan         nan\n",
      "   0.16461473]\n",
      " [ 1.          0.34213833 -0.86782304 ...         nan         nan\n",
      "   1.37945365]\n",
      " [ 1.          0.09260008  0.7554813  ...         nan         nan\n",
      "  -1.04963898]]\n",
      "(25000, 31)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  1.85013498e-01  9.50020870e-01 ... -3.65927885e-03\n",
      "  -7.92317380e-04 -1.19024578e+00]\n",
      " [ 1.00000000e+00 -3.86529457e+00  4.13705283e-01 ... -3.65927885e-03\n",
      "  -7.92317380e-04  5.24128170e-01]\n",
      " [ 1.00000000e+00 -1.19408579e+00  1.23030549e+00 ... -6.66781959e-01\n",
      "   8.78429685e-01  6.74525217e-01]\n",
      " ...\n",
      " [ 1.00000000e+00 -1.69457506e+00  2.25961191e-01 ... -3.65927885e-03\n",
      "  -7.92317380e-04  5.34849892e-01]\n",
      " [ 1.00000000e+00  4.37396292e-01 -6.55445237e-01 ... -2.05217222e-01\n",
      "  -2.23709453e+00  1.13299914e+00]\n",
      " [ 1.00000000e+00  1.85013498e-01  7.36123776e-01 ... -3.65927885e-03\n",
      "  -7.92317380e-04 -1.19024578e+00]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-31.682259546099825"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(weight, x_test, boundary = 0.5):\n",
    "    \"\"\"\n",
    "    # Gives predictions given weight and datapoints \n",
    "    \n",
    "    :param weight: vector weight\n",
    "    :type weight: 1D array\n",
    "    \n",
    "    :param x_test: extended feature matrix\n",
    "    :type x_test: 2D array\n",
    "    \n",
    "    :return: label predictions (0 or 1)\n",
    "    :rtype:  1D numpy array\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pred = x_test.dot(weight)\n",
    "    return (pred > boundary)*1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0860633628410993\n"
     ]
    }
   ],
   "source": [
    "w, loss = ridge_regression(y_train, x_train,0.004)\n",
    "#print(w) # gives an idea about the important columns.. \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def K_Cross_Validation(x, y, K, _lambda):\n",
    "    #Initialization of all needed arrays\n",
    "    test_loss = np.zeros(K)\n",
    "    train_loss = np.zeros(K)\n",
    "    weights = np.zeros((K,x.shape[1]))\n",
    "    accuracy = np.zeros(K)\n",
    "    indices = build_k_indices(y, K)\n",
    "    \n",
    "    for i in range(K):\n",
    "        test_indices = indices[i]\n",
    "        y_test = y[test_indices]\n",
    "        y_train = np.delete(y,test_indices)\n",
    "        x_test = x[test_indices,:]\n",
    "        x_train = np.delete(x,test_indices,axis=0)\n",
    "        ### ADAPT METHOD & LOSS\n",
    "        weights[i], train_loss[i] = ridge_regression(y_train, x_train,_lambda)\n",
    "        test_loss[i] = compute_ridge_loss(y_test,x_test,weights[i],_lambda)\n",
    "        \n",
    "        #Calculate predictions of the model\n",
    "        predictions = predict(weights[i] , x_test)\n",
    "        #Calculate accuracy of the model\n",
    "        accuracy[i] = np.sum(predictions == y_test) / len(y_test)\n",
    "        \n",
    "    return accuracy, test_loss, train_loss, np.mean(weights, axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7404088888888889\n",
      "[0.08547195 0.08567905 0.08480681 0.08555782]\n",
      "[0.08531411 0.08524682 0.08553647 0.08528475]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accs,test_loss, train_loss, w = K_Cross_Validation(x_train,y_train,4,0.001)\n",
    "plt.boxplot(accs)\n",
    "print(accs.mean())\n",
    "print(test_loss)\n",
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal acc = 0.7401333333333333 with lambda= 1e-07\n"
     ]
    }
   ],
   "source": [
    "def Tune_lambda(xt_training, y_training, K, gamma_range):\n",
    "        \n",
    "    lambdas = np.logspace(-7, gamma_range, 10)\n",
    "    max_acc = 0\n",
    "    min_loss = np.inf\n",
    "    opt_lambda = 0\n",
    "    accuracies = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for i, lambda_ in enumerate(lambdas):\n",
    "       \n",
    "        accuracy,test,train,w = K_Cross_Validation(xt_training, y_training, K,lambda_)\n",
    "        accuracies.append([lambda_,np.median(accuracy)])\n",
    "        train_losses.append([lambda_,np.median(train)])\n",
    "        test_losses.append([lambda_,np.median(test)])\n",
    "        if (np.median(test) < min_loss):\n",
    "            min_loss = np.median(test)\n",
    "            max_acc = np.median(accuracy)\n",
    "            opt_lambda = lambda_\n",
    "                \n",
    "    return opt_lambda , max_acc, np.array(accuracies), np.array(train_losses), np.array(test_losses)\n",
    "opt_lambda, max_acc, acc ,train, test= Tune_lambda(x_train, y_train, 5, 1)\n",
    "\n",
    "print(f\"optimal acc = {max_acc} with lambda= {opt_lambda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on Test Set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_opt,loss = ridge_regression(y_train,x_train,0.00001)\n",
    "p = predict(w_opt,x_test)\n",
    "print((p==y_test).mean())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
