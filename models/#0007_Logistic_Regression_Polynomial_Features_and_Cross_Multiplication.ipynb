{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T07:55:00.512020Z",
     "start_time": "2020-10-26T07:54:59.532458Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# add project root folder to path to allow import local modules\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "# import local modules\n",
    "from helpers import compute_ridge_loss\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T07:55:00.516853Z",
     "start_time": "2020-10-26T07:55:00.512837Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T07:55:02.579312Z",
     "start_time": "2020-10-26T07:55:02.545404Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_where_function_is_almost_constant(data, epsilon, is_data_sorted = False):\n",
    "    diff_data = np.diff(data)\n",
    "    if (is_data_sorted):\n",
    "        return np.searchsorted(abs(diff_data) <=epsilon)\n",
    "    indices = np.where(abs(diff_data) <= epsilon)[0]\n",
    "    if len(indices) == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return indices[0]\n",
    "\n",
    "def generate_different_gammas(i, linear_gamma):\n",
    "    # set gamma (learning rate) \n",
    "    if (linear_gamma):\n",
    "        # linear gamma\n",
    "        gamma_0 = 10 ** (-i)\n",
    "        start_gamma = gamma_0\n",
    "        stop_gamma = 10*gamma_0\n",
    "        step_gamma = 1*gamma_0\n",
    "        gammas = np.arange(start_gamma, stop_gamma, step_gamma)\n",
    "    else:\n",
    "        # exponential gamma\n",
    "        start_degree = -i * 10 \n",
    "        stop_degree = -(i-1) * 10\n",
    "        number_of_points = stop_degree - start_degree + 1\n",
    "        gammas = np.logspace(start_degree, stop_degree, number_of_points)\n",
    "        \n",
    "        start_gamma = 10**start_degree\n",
    "        stop_gamma = 10**stop_degree\n",
    "        step_gamma = 10**number_of_points\n",
    "        \n",
    "    FRACTION_PRECISION = 1\n",
    "    gamma_fractions = np.array([[1/x, x] for x in range(1, FRACTION_PRECISION + 1)])\n",
    "    gamma_fractions  = gamma_fractions .reshape(np.size(gamma_fractions))\n",
    "    \n",
    "    \n",
    "    gammas = np.array([[gamma_fractions * gamma] for gamma in gammas]) \n",
    "    gammas = gammas.reshape(np.size(gammas)) # put into a huge vector/array\n",
    "    gammas = np.sort(gammas) # sort it\n",
    "    gammas = np.unique(gammas)\n",
    "    return gammas\n",
    "def plot_cost_max_iter(i,y,x,regression_type,lambda_ = 0):\n",
    "    # generate gammas\n",
    "    gammas = generate_different_gammas(i, linear_gamma = False)\n",
    "    \n",
    "    # set maximum number of iterations\n",
    "    MAXIMUM_NUMBER_OF_ITERATIONS = 100\n",
    "    start_max_iters = 1\n",
    "    stop_max_iters = MAXIMUM_NUMBER_OF_ITERATIONS+1\n",
    "    step_max_iters = 1\n",
    "    max_iters = np.arange(start_max_iters, stop_max_iters, step_max_iters)\n",
    "    \n",
    "    # add 1s to the first column of x\n",
    "    #tx = make_tx(x) # add ones\n",
    "    tx = x\n",
    "    # extract dimensions\n",
    "    N,D = np.shape(tx)\n",
    "    # initial vector of values for weights\n",
    "    initial_w = np.zeros(D) \n",
    "    # save value of loss\\cost function\n",
    "    all_losses = []\n",
    "    # save value of weights for different models\n",
    "    all_weights = []\n",
    "    # for each gamma save the location where the loss curve\n",
    "    # is almost flat, or in other words where the difference of\n",
    "    # two consecutive points is smaller then EPSILON\n",
    "    EPSILON = 1e-5\n",
    "    all_max_iterations_with_epsilon_precision = []\n",
    "    plt.figure()\n",
    "    for gamma in gammas:\n",
    "        all_losses = []\n",
    "        # model(hypothesis) is reg. logistic regression\n",
    "        if (regression_type == \"reg_logistic_regression\"):\n",
    "            w,losses = reg_logistic_regression(y, tx, lambda_, initial_w, MAXIMUM_NUMBER_OF_ITERATIONS, gamma, True)\n",
    "        # model(hypothesis) is logistic regression\n",
    "        if (regression_type == \"logistic_regression\"):\n",
    "            w,losses = logistic_regression(y, tx, initial_w, MAXIMUM_NUMBER_OF_ITERATIONS, gamma, True)\n",
    "        \n",
    "        # IF LOSS FUNCTION IS INCREASING THEN WE HAVE TO BIG GAMMA\n",
    "        # and because gamma is increasing here, that means it will be even\n",
    "        # bigger and bigger; therefore, we terminate for loop\n",
    "        # and plot all previous loss functions for different gammmas\n",
    "        if (not np.all(np.diff(losses)<=0)):\n",
    "            gammas = gammas[:(np.where(gammas == gamma)[0][0] - 1)]\n",
    "            break\n",
    "        \n",
    "        # save the value of the loss function for fixed gamma and max_iter\n",
    "        all_losses.append(losses)\n",
    "        \n",
    "        # save weights when the loss function is the smalles, or in other words\n",
    "        # save weights with the biggest number max_iter\n",
    "        all_weights.append(np.array(w))\n",
    "        \n",
    "        # plot maximum number of iterations (x-axis) and cost/\n",
    "        plt.plot(max_iters,losses)# , label =\"gamma = \"+str(np.round(gamma)))\n",
    "        \n",
    "        # save for each gamma the number of maximum iterations\n",
    "        # where the value doesnt change \n",
    "        #all_max_iterations_with_epsilon_precision.append(find_where_function_is_almost_constant(losses, EPSILON,False))\n",
    "        \n",
    "    plt.ylabel('Value of cost/loss function')\n",
    "    plt.xlabel('Max. number of iterations')\n",
    "    plt.title('Cost functions for different gammas over of max. number of iterations (epsilon = ' + str(EPSILON) + ')\\n')\n",
    "    plt.grid(True, 'both')\n",
    "    \n",
    "    #plt.legend(handles=[p1, p2], title='title', , loc='upper left', prop=fontP)\n",
    "    #plt.legend([p1, p2], , bbox_to_anchor=(1.05, 1), , prop=fontP)\n",
    "    #LEGEND_TAG = [['gamma_ ' + str(j) +' = '+str(gammas[j]) + ' //  ' + str(all_max_iterations_with_epsilon_precision[j])] for j in range(len(gammas))] \n",
    "    \n",
    "    LEGEND_TAG = [['gamma_ ' + str(j) +' = '+str(gammas[j])] for j in range(len(gammas))] \n",
    "    plt.legend(LEGEND_TAG,title='LEGEND\\n Gamma_i //  Constant after max_inter_i\\n (-1 means function is not const for given epsilon)',bbox_to_anchor=(1.8, 1),loc='upper right')\n",
    "    #plt.legend()\n",
    "    plt.xlim(start_max_iters, stop_max_iters)     # set the xlim to left, right\n",
    "    # plt.yscale(\"log\")\n",
    "    \n",
    "    plt.savefig(regression_type+'_image_'+str(i)+'.png',bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"GAMMAS: \",gammas)\n",
    "    print(\"The function is constant (two points are less than epsilon = \"+str(EPSILON)+\" ) after each max_iteration (for each gamma, respectively): \\n\", all_max_iterations_with_epsilon_precision, \"\\n  ~(-1 means that function is never constant for a given plot)~ \")\n",
    "    #return np.array(all_weights), gammas\n",
    "    return np.array(all_weights), np.array(all_losses), gammas, all_max_iterations_with_epsilon_precision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T07:55:04.160719Z",
     "start_time": "2020-10-26T07:55:04.156885Z"
    }
   },
   "outputs": [],
   "source": [
    "DEGREES = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make custom pipeline to create polynomial expansion of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T07:55:06.383515Z",
     "start_time": "2020-10-26T07:55:06.373510Z"
    }
   },
   "outputs": [],
   "source": [
    "class PolyPreprocessing(Preprocessing):\n",
    "    def __init__(self,dataset):\n",
    "        super().__init__(dataset)\n",
    "        self.degree = None\n",
    "        self.cross_degree = None\n",
    "        \n",
    "    def set_degrees(self,degree, cross_degree):\n",
    "        self.degree = degree\n",
    "        self.cross_degree = cross_degree\n",
    "        \n",
    "    def _feature_engineering(self):\n",
    "        super()._feature_engineering() # to create pipeline\n",
    "        \n",
    "        dataset =self.dataset\n",
    "        \n",
    "        for deg in range(2,self.degree+1):\n",
    "            self.dataset = np.concatenate((self.dataset, np.power(dataset,deg)),axis=1)\n",
    "        \n",
    "        if (self.cross_degree):\n",
    "            for col_i in range(dataset.shape[1]):\n",
    "                print(col_i)\n",
    "                for col_j in range(col_i+1,dataset.shape[1]):\n",
    "                    col = dataset[:,col_i]*dataset[:,col_j]\n",
    "                    self.dataset = np.concatenate((self.dataset,col.reshape((-1,1))),axis=1)\n",
    "\n",
    "                    \n",
    "class AddFeaturesPolyPreprocessing(PolyPreprocessing):\n",
    "    def __init__(self,dataset):\n",
    "        super().__init__(dataset)\n",
    "    def _feature_engineering(self):\n",
    "        super()._feature_engineering()\n",
    "        X = np.array(self.original_dataset.tolist()) # make unstructured, not very efficient..\n",
    "        X = X[:,2:] # remove IDs and '?' of predictions\n",
    "        col = X[:,1]\n",
    "        f1 = col = 1-np.exp(-col**2/5000).reshape((-1,1))\n",
    "        self.dataset = np.concatenate((self.dataset,f1),axis=1)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T07:55:52.479537Z",
     "start_time": "2020-10-26T07:55:41.326014Z"
    }
   },
   "outputs": [],
   "source": [
    "p_train = AddFeaturesPolyPreprocessing(load_csv('../dataset/trainset.csv'))\n",
    "p_test = AddFeaturesPolyPreprocessing(load_csv('../dataset/testset.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T08:00:04.309416Z",
     "start_time": "2020-10-26T07:55:52.480526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-ea50ab9e72b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mp_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_degrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEGREES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mp_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_degrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEGREES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mp_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML-Project1/preprocessing.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, replace_missing_by_mean, outlier_removal, save_y_and_tX, labeled)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_engineering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutlier_removal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-577a27c7e574>\u001b[0m in \u001b[0;36m_feature_engineering\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_feature_engineering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_engineering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# make unstructured, not very efficient..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# remove IDs and '?' of predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-577a27c7e574>\u001b[0m in \u001b[0;36m_feature_engineering\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcol_j\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p_train.set_degrees(DEGREES,1)\n",
    "p_test.set_degrees(DEGREES,1)\n",
    "y_train , x_train= p_train.preprocess()\n",
    "y_test, x_test = p_test.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T08:07:19.317860Z",
     "start_time": "2020-10-26T08:07:19.304894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225000, 707)\n",
      "(25000, 707)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_no_bias = x_train[:,1: ]\n",
    "x_test_no_bias = x_test[:,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= np.concatenate((np.ones(x_train.shape[0]).reshape(-1,1), x_train_no_bias), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.concatenate((np.ones(x_test.shape[0]).reshape(-1,1), x_test_no_bias), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00 -8.92813485e-15  1.15995991e+00 ...  2.45926277e-04\n",
      "   3.97590008e-03  1.36053306e+00]\n",
      " [ 1.00000000e+00 -1.76965057e+00  8.19004917e-02 ...  2.45926277e-04\n",
      "   3.97590008e-03  1.15223584e-01]\n",
      " [ 1.00000000e+00 -8.96103179e-01  2.00269249e+00 ...  6.53428651e-03\n",
      "  -3.92418444e-03  1.81611569e+00]\n",
      " ...\n",
      " [ 1.00000000e+00 -1.12767100e+00 -1.77347334e-01 ...  2.45926277e-04\n",
      "   3.97590008e-03 -2.42335243e-01]\n",
      " [ 1.00000000e+00  2.16522338e-01 -9.09998070e-01 ... -1.06881971e-01\n",
      "  -1.91580451e+00 -1.05738577e+00]\n",
      " [ 1.00000000e+00 -8.92813485e-15  6.59291625e-01 ...  2.45926277e-04\n",
      "   3.97590008e-03  8.64872650e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T08:10:45.512031Z",
     "start_time": "2020-10-26T08:10:43.442526Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( x_train, open( f\"0005_deg{DEGREES}_x_train.p\", \"wb\" ) )\n",
    "pickle.dump( x_test, open( f\"0005_deg{DEGREES}_x_test.p\", \"wb\" ) )\n",
    "pickle.dump( y_train, open( f\"0005_deg{DEGREES}_y_train.p\", \"wb\" ) )\n",
    "pickle.dump( y_test, open( f\"0005_deg{DEGREES}_y_test.p\", \"wb\" ) )\n",
    "#pickle.dump( tx, open(f\"0005_deg{DEGREES}_x_predict.p\",\"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T08:02:32.410601Z",
     "start_time": "2020-10-26T08:02:32.405604Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(weight, x_test,boundary =0.5):\n",
    "    \"\"\"\n",
    "    # Gives predictions given weight and datapoints \n",
    "    \n",
    "    :param weight: vector weight\n",
    "    :type weight: 1D array\n",
    "    \n",
    "    :param x_test: extended feature matrix\n",
    "    :type x_test: 2D array\n",
    "    \n",
    "    :return: label predictions (0 or 1)\n",
    "    :rtype:  1D numpy array\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pred = sigmoid(x_test.dot(weight))\n",
    "    \n",
    "    f = lambda x : 0 if x <boundary else 1\n",
    "    \n",
    "    predictions = np.array([ f(x) for x in pred])\n",
    "    \n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_reg, losses_reg = reg_logistic_regression(y_train, x_train,lambda_,  initial_w, max_iters, gamma, all_losses=True) \n",
    "for i in range(1,8):\n",
    "    plt.figure(i)\n",
    "    plot_cost_max_iter(i, y_train,x_train,\"logistic_regression\",lambda_ = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0\n",
    "#w, loss = ridge_regression(y_train, x_train,lambda_=0.004)\n",
    "#print(loss)\n",
    "\n",
    "N,D = x_train.shape\n",
    "initial_w = np.zeros(D)\n",
    "max_iters = 10\n",
    "gamma = 1*1e-25\n",
    "\n",
    "w_reg, losses_reg = reg_logistic_regression(y_train, x_train,lambda_,  initial_w, max_iters, gamma, all_losses=True)\n",
    "\n",
    "print(np.diff(losses_reg) <= 0)\n",
    "print(np.round(losses_reg,7))\n",
    "\n",
    "w, losses = logistic_regression(y_train, x_train, initial_w, max_iters, gamma, all_losses=True)\n",
    "\n",
    "print(np.diff(losses) <= 0)\n",
    "print(np.round(losses,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T15:00:03.853432Z",
     "start_time": "2020-10-18T15:00:02.657630Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_ = 20\n",
    "#w, loss = ridge_regression(y_train, x_train,lambda_=0.004)\n",
    "#print(loss)\n",
    "\n",
    "N,D = x_train.shape\n",
    "initial_w = np.zeros(D)\n",
    "max_iters = 10\n",
    "gamma = 1*1e-25\n",
    "\n",
    "w_reg, losses_reg = reg_logistic_regression(y_train, x_train,lambda_,  initial_w, max_iters, gamma, all_losses=True)\n",
    "\n",
    "print(np.diff(losses_reg) <= 0)\n",
    "print(np.round(losses_reg,7))\n",
    "\n",
    "w, losses = logistic_regression(y_train, x_train, initial_w, max_iters, gamma, all_losses=True)\n",
    "\n",
    "print(np.diff(losses) <= 0)\n",
    "print(np.round(losses,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T09:42:57.687146Z",
     "start_time": "2020-10-17T09:42:57.684154Z"
    }
   },
   "source": [
    "##  Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T15:00:03.866396Z",
     "start_time": "2020-10-18T15:00:03.855427Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def K_Cross_Validation(x, y, K,gamma = 0):\n",
    "    #Initialization of all needed arrays\n",
    "    test_loss = np.zeros(K)\n",
    "    train_loss = np.zeros(K)\n",
    "    weights = np.zeros((K,x.shape[1]))\n",
    "    accuracy = np.zeros(K)\n",
    "    indices = build_k_indices(y, K)\n",
    "    max_iters = 10000\n",
    "    N,D = x.shape\n",
    "    initial_w = np.zeros(D)\n",
    "    for i in range(K):\n",
    "        test_indices = indices[i]\n",
    "        y_test = y[test_indices]\n",
    "        y_train = np.delete(y,test_indices)\n",
    "        x_test = x[test_indices,:]\n",
    "        x_train = np.delete(x,test_indices,axis=0)\n",
    "        ### ADAPT METHOD & LOSS\n",
    "        weights[i], train_loss[i] =  logistic_regression(y_train, x_train, initial_w, 100, 1*1e-15, all_losses=False)\n",
    "        #weights[i], train_loss[i] = reg_batch_logistic_regression(y_train, x_train,1*1e-15, initial_w, max_iters, gamma,20 )\n",
    "        #test_loss[i] =  compute_ridge_loss(y_test,x_test,weights[i],_lambda)\n",
    "        test_loss[i] = loss_of_logistic_regression(x_test,y_test,weights[i])\n",
    "        \n",
    "        \n",
    "        #Calculate predictions of the model\n",
    "        predictions = predict(weights[i] , x_test)\n",
    "        #Calculate accuracy of the model\n",
    "        accuracy[i] = np.sum(predictions == y_test) / len(y_test)\n",
    "        \n",
    "    return accuracy, test_loss, train_loss, np.mean(weights, axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "x_train = pickle.load(open( f\"0005_deg9_x_train.p\", \"rb\" ) )\n",
    "x_test = pickle.load( open( f\"0005_deg9_x_test.p\", \"rb\" ) )\n",
    "y_train = pickle.load(open( f\"0005_deg9_y_train.p\", \"rb\" ) )\n",
    "y_test = pickle.load( open( f\"0005_deg9_y_test.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T15:00:10.616318Z",
     "start_time": "2020-10-18T15:00:03.868392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7080044444444444\n",
      "[38989.52825598 38989.52825269 38989.52825481 38989.52824901]\n",
      "[116968.58474778 116968.5847575  116968.58475159 116968.5847684 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accs,test_loss, train_loss, w = K_Cross_Validation(x_train,y_train,4, 0.001)\n",
    "plt.boxplot(accs)\n",
    "print(accs.mean())\n",
    "print(test_loss)\n",
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T15:01:50.713442Z",
     "start_time": "2020-10-18T15:00:10.617261Z"
    }
   },
   "outputs": [],
   "source": [
    "#def Tune_lambda(xt_training, y_training, K, gamma_range):\n",
    "def Tune_gamma(xt_training, y_training, K, gamma_range = 31):  \n",
    "    start_power = -40\n",
    "    stop_power = -10\n",
    "    gammas = np.logspace(start_power, stop_power, gamma_range)\n",
    "    max_acc = 0\n",
    "    min_loss = np.inf\n",
    "    #opt_lambda = 0\n",
    "    opt_gamma = 1e-40\n",
    "    accuracies = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for i, gamma in enumerate(gammas):\n",
    "        accuracy,test,train,w = K_Cross_Validation(xt_training, y_training, K, gamma)\n",
    "        accuracies.append([gamma,np.median(accuracy)])\n",
    "        train_losses.append([gamma,np.median(train)])\n",
    "        test_losses.append([gamma,np.median(test)])\n",
    "        if (np.median(test) < min_loss):\n",
    "            min_loss = np.median(test)\n",
    "            max_acc = np.median(accuracy)\n",
    "            opt_gamma = gamma\n",
    "                \n",
    "    return opt_gamma , max_acc, np.array(accuracies), np.array(train_losses), np.array(test_losses)\n",
    "opt_gamma, max_acc, acc ,train, test= Tune_gamma(x_train, y_train, 5,11)\n",
    "\n",
    "print(f\"optimal acc = {max_acc} with gamma= {opt_gamma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T15:01:51.375638Z",
     "start_time": "2020-10-18T15:01:50.714443Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(acc[:,0],acc[:,1], label=\"accuracy\")\n",
    "plt.plot(train[:,0],train[:,1],alpha=0.5,label=\"train error\")\n",
    "plt.plot(test[:,0],test[:,1],alpha=0.5, label = \"test eror\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"MSE/accuracy\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Logistic regression - Tuning the learning rate gamma (logarithmic scale)\")\n",
    "plt.savefig('Acuracy_logistic_regression.png',bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predictions on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T08:06:43.087742Z",
     "start_time": "2020-10-26T08:06:14.343034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70916\n",
      "17328.6794683917\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(x_train.shape[1])\n",
    "gamma = 0.001\n",
    "max_iters = 1000\n",
    "\n",
    "w_opt,loss = logistic_regression(y_test, x_test, initial_w, 100, 1*1e-15, all_losses=False)\n",
    "\n",
    "#w_opt,loss = reg_batch_logistic_regression(y_test, x_test,1*1e-15, initial_w, max_iters, gamma,20 )\n",
    "p = predict(w_opt,x_test)\n",
    "print((p==y_test).mean())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88115977, 0.11884023],\n",
       "       [0.29173949, 0.70826051]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
