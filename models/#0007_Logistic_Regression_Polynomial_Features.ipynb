{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T07:55:00.512020Z",
     "start_time": "2020-10-26T07:54:59.532458Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# add project root folder to path to allow import local modules\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "# import local modules\n",
    "from helpers import compute_ridge_loss\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T07:55:00.516853Z",
     "start_time": "2020-10-26T07:55:00.512837Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T07:55:02.579312Z",
     "start_time": "2020-10-26T07:55:02.545404Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_where_function_is_almost_constant(data, epsilon, is_data_sorted = False):\n",
    "    diff_data = np.diff(data)\n",
    "    if (is_data_sorted):\n",
    "        return np.searchsorted(abs(diff_data) <=epsilon)\n",
    "    indices = np.where(abs(diff_data) <= epsilon)[0]\n",
    "    if len(indices) == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return indices[0]\n",
    "\n",
    "def generate_different_gammas(i, linear_gamma):\n",
    "    # set gamma (learning rate) \n",
    "    if (linear_gamma):\n",
    "        # linear gamma\n",
    "        gamma_0 = 10 ** (-i)\n",
    "        start_gamma = gamma_0\n",
    "        stop_gamma = 10*gamma_0\n",
    "        step_gamma = 1*gamma_0\n",
    "        gammas = np.arange(start_gamma, stop_gamma, step_gamma)\n",
    "    else:\n",
    "        # exponential gamma\n",
    "        start_degree = -i * 10 \n",
    "        stop_degree = -(i-1) * 10\n",
    "        number_of_points = stop_degree - start_degree + 1\n",
    "        gammas = np.logspace(start_degree, stop_degree, number_of_points)\n",
    "        \n",
    "        start_gamma = 10**start_degree\n",
    "        stop_gamma = 10**stop_degree\n",
    "        step_gamma = 10**number_of_points\n",
    "        \n",
    "    FRACTION_PRECISION = 1\n",
    "    gamma_fractions = np.array([[1/x, x] for x in range(1, FRACTION_PRECISION + 1)])\n",
    "    gamma_fractions  = gamma_fractions .reshape(np.size(gamma_fractions))\n",
    "    \n",
    "    \n",
    "    gammas = np.array([[gamma_fractions * gamma] for gamma in gammas]) \n",
    "    gammas = gammas.reshape(np.size(gammas)) # put into a huge vector/array\n",
    "    gammas = np.sort(gammas) # sort it\n",
    "    gammas = np.unique(gammas)\n",
    "    return gammas\n",
    "def plot_cost_max_iter(i,y,x,regression_type,lambda_ = 0):\n",
    "    # generate gammas\n",
    "    gammas = generate_different_gammas(i, linear_gamma = False)\n",
    "    \n",
    "    # set maximum number of iterations\n",
    "    MAXIMUM_NUMBER_OF_ITERATIONS = 100\n",
    "    start_max_iters = 1\n",
    "    stop_max_iters = MAXIMUM_NUMBER_OF_ITERATIONS+1\n",
    "    step_max_iters = 1\n",
    "    max_iters = np.arange(start_max_iters, stop_max_iters, step_max_iters)\n",
    "    \n",
    "    # add 1s to the first column of x\n",
    "    #tx = make_tx(x) # add ones\n",
    "    tx = x\n",
    "    # extract dimensions\n",
    "    N,D = np.shape(tx)\n",
    "    # initial vector of values for weights\n",
    "    initial_w = np.zeros(D) \n",
    "    # save value of loss\\cost function\n",
    "    all_losses = []\n",
    "    # save value of weights for different models\n",
    "    all_weights = []\n",
    "    # for each gamma save the location where the loss curve\n",
    "    # is almost flat, or in other words where the difference of\n",
    "    # two consecutive points is smaller then EPSILON\n",
    "    EPSILON = 1e-5\n",
    "    all_max_iterations_with_epsilon_precision = []\n",
    "    plt.figure()\n",
    "    for gamma in gammas:\n",
    "        all_losses = []\n",
    "        # model(hypothesis) is reg. logistic regression\n",
    "        if (regression_type == \"reg_logistic_regression\"):\n",
    "            w,losses = reg_logistic_regression(y, tx, lambda_, initial_w, MAXIMUM_NUMBER_OF_ITERATIONS, gamma, True)\n",
    "        # model(hypothesis) is logistic regression\n",
    "        if (regression_type == \"logistic_regression\"):\n",
    "            w,losses = logistic_regression(y, tx, initial_w, MAXIMUM_NUMBER_OF_ITERATIONS, gamma, True)\n",
    "        \n",
    "        # IF LOSS FUNCTION IS INCREASING THEN WE HAVE TO BIG GAMMA\n",
    "        # and because gamma is increasing here, that means it will be even\n",
    "        # bigger and bigger; therefore, we terminate for loop\n",
    "        # and plot all previous loss functions for different gammmas\n",
    "        if (not np.all(np.diff(losses)<=0)):\n",
    "            gammas = gammas[:(np.where(gammas == gamma)[0][0] - 1)]\n",
    "            break\n",
    "        \n",
    "        # save the value of the loss function for fixed gamma and max_iter\n",
    "        all_losses.append(losses)\n",
    "        \n",
    "        # save weights when the loss function is the smalles, or in other words\n",
    "        # save weights with the biggest number max_iter\n",
    "        all_weights.append(np.array(w))\n",
    "        \n",
    "        # plot maximum number of iterations (x-axis) and cost/\n",
    "        plt.plot(max_iters,losses)# , label =\"gamma = \"+str(np.round(gamma)))\n",
    "        \n",
    "        # save for each gamma the number of maximum iterations\n",
    "        # where the value doesnt change \n",
    "        #all_max_iterations_with_epsilon_precision.append(find_where_function_is_almost_constant(losses, EPSILON,False))\n",
    "        \n",
    "    plt.ylabel('Value of cost/loss function')\n",
    "    plt.xlabel('Max. number of iterations')\n",
    "    plt.title('Cost functions for different gammas over of max. number of iterations (epsilon = ' + str(EPSILON) + ')\\n')\n",
    "    plt.grid(True, 'both')\n",
    "    \n",
    "    #plt.legend(handles=[p1, p2], title='title', , loc='upper left', prop=fontP)\n",
    "    #plt.legend([p1, p2], , bbox_to_anchor=(1.05, 1), , prop=fontP)\n",
    "    #LEGEND_TAG = [['gamma_ ' + str(j) +' = '+str(gammas[j]) + ' //  ' + str(all_max_iterations_with_epsilon_precision[j])] for j in range(len(gammas))] \n",
    "    \n",
    "    LEGEND_TAG = [['gamma_ ' + str(j) +' = '+str(gammas[j])] for j in range(len(gammas))] \n",
    "    plt.legend(LEGEND_TAG,title='LEGEND\\n Gamma_i //  Constant after max_inter_i\\n (-1 means function is not const for given epsilon)',bbox_to_anchor=(1.8, 1),loc='upper right')\n",
    "    #plt.legend()\n",
    "    plt.xlim(start_max_iters, stop_max_iters)     # set the xlim to left, right\n",
    "    # plt.yscale(\"log\")\n",
    "    \n",
    "    plt.savefig(regression_type+'_image_'+str(i)+'.png',bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"GAMMAS: \",gammas)\n",
    "    print(\"The function is constant (two points are less than epsilon = \"+str(EPSILON)+\" ) after each max_iteration (for each gamma, respectively): \\n\", all_max_iterations_with_epsilon_precision, \"\\n  ~(-1 means that function is never constant for a given plot)~ \")\n",
    "    #return np.array(all_weights), gammas\n",
    "    return np.array(all_weights), np.array(all_losses), gammas, all_max_iterations_with_epsilon_precision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T07:55:04.160719Z",
     "start_time": "2020-10-26T07:55:04.156885Z"
    }
   },
   "outputs": [],
   "source": [
    "DEGREES = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make custom pipeline to create polynomial expansion of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T07:55:06.383515Z",
     "start_time": "2020-10-26T07:55:06.373510Z"
    }
   },
   "outputs": [],
   "source": [
    "class PolyPreprocessing(Preprocessing):\n",
    "    def __init__(self,dataset):\n",
    "        super().__init__(dataset)\n",
    "        self.degree = None\n",
    "        self.cross_degree = None\n",
    "        \n",
    "    def set_degrees(self,degree, cross_degree):\n",
    "        self.degree = degree\n",
    "        self.cross_degree = cross_degree\n",
    "        \n",
    "    def _feature_engineering(self):\n",
    "        super()._feature_engineering() # to create pipeline\n",
    "        \n",
    "        dataset =self.dataset\n",
    "        \n",
    "        for deg in range(2,self.degree+1):\n",
    "            self.dataset = np.concatenate((self.dataset, np.power(dataset,deg)),axis=1)\n",
    "        \n",
    "        if (self.cross_degree):\n",
    "            for col_i in range(dataset.shape[1]):\n",
    "                print(col_i)\n",
    "                for col_j in range(col_i+1,dataset.shape[1]):\n",
    "                    col = dataset[:,col_i]*dataset[:,col_j]\n",
    "                    self.dataset = np.concatenate((self.dataset,col.reshape((-1,1))),axis=1)\n",
    "\n",
    "                    \n",
    "class AddFeaturesPolyPreprocessing(PolyPreprocessing):\n",
    "    def __init__(self,dataset):\n",
    "        super().__init__(dataset)\n",
    "    def _feature_engineering(self):\n",
    "        super()._feature_engineering()\n",
    "        X = np.array(self.original_dataset.tolist()) # make unstructured, not very efficient..\n",
    "        X = X[:,2:] # remove IDs and '?' of predictions\n",
    "        col = X[:,1]\n",
    "        f1 = col = 1-np.exp(-col**2/5000).reshape((-1,1))\n",
    "        self.dataset = np.concatenate((self.dataset,f1),axis=1)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T07:55:52.479537Z",
     "start_time": "2020-10-26T07:55:41.326014Z"
    }
   },
   "outputs": [],
   "source": [
    "p_train = AddFeaturesPolyPreprocessing(load_csv('../dataset/trainset.csv'))\n",
    "p_test = AddFeaturesPolyPreprocessing(load_csv('../dataset/testset.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T08:00:04.309416Z",
     "start_time": "2020-10-26T07:55:52.480526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-ea50ab9e72b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mp_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_degrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEGREES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mp_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_degrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEGREES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mp_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML-Project1/preprocessing.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, replace_missing_by_mean, outlier_removal, save_y_and_tX, labeled)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_engineering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutlier_removal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-577a27c7e574>\u001b[0m in \u001b[0;36m_feature_engineering\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_feature_engineering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_engineering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# make unstructured, not very efficient..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# remove IDs and '?' of predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-577a27c7e574>\u001b[0m in \u001b[0;36m_feature_engineering\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcol_j\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p_train.set_degrees(DEGREES,1)\n",
    "p_test.set_degrees(DEGREES,1)\n",
    "y_train , x_train= p_train.preprocess()\n",
    "y_test, x_test = p_test.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T08:07:19.317860Z",
     "start_time": "2020-10-26T08:07:19.304894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225000, 707)\n",
      "(25000, 707)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_no_bias = x_train[:,1: ]\n",
    "x_test_no_bias = x_test[:,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= np.concatenate((np.ones(x_train.shape[0]).reshape(-1,1), x_train_no_bias), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.concatenate((np.ones(x_test.shape[0]).reshape(-1,1), x_test_no_bias), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00 -8.92813485e-15  1.15995991e+00 ...  2.45926277e-04\n",
      "   3.97590008e-03  1.36053306e+00]\n",
      " [ 1.00000000e+00 -1.76965057e+00  8.19004917e-02 ...  2.45926277e-04\n",
      "   3.97590008e-03  1.15223584e-01]\n",
      " [ 1.00000000e+00 -8.96103179e-01  2.00269249e+00 ...  6.53428651e-03\n",
      "  -3.92418444e-03  1.81611569e+00]\n",
      " ...\n",
      " [ 1.00000000e+00 -1.12767100e+00 -1.77347334e-01 ...  2.45926277e-04\n",
      "   3.97590008e-03 -2.42335243e-01]\n",
      " [ 1.00000000e+00  2.16522338e-01 -9.09998070e-01 ... -1.06881971e-01\n",
      "  -1.91580451e+00 -1.05738577e+00]\n",
      " [ 1.00000000e+00 -8.92813485e-15  6.59291625e-01 ...  2.45926277e-04\n",
      "   3.97590008e-03  8.64872650e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T08:10:45.512031Z",
     "start_time": "2020-10-26T08:10:43.442526Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( x_train, open( f\"0005_deg{DEGREES}_x_train.p\", \"wb\" ) )\n",
    "pickle.dump( x_test, open( f\"0005_deg{DEGREES}_x_test.p\", \"wb\" ) )\n",
    "pickle.dump( y_train, open( f\"0005_deg{DEGREES}_y_train.p\", \"wb\" ) )\n",
    "pickle.dump( y_test, open( f\"0005_deg{DEGREES}_y_test.p\", \"wb\" ) )\n",
    "#pickle.dump( tx, open(f\"0005_deg{DEGREES}_x_predict.p\",\"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T08:02:32.410601Z",
     "start_time": "2020-10-26T08:02:32.405604Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(weight, x_test,boundary =0.5):\n",
    "    \"\"\"\n",
    "    # Gives predictions given weight and datapoints \n",
    "    \n",
    "    :param weight: vector weight\n",
    "    :type weight: 1D array\n",
    "    \n",
    "    :param x_test: extended feature matrix\n",
    "    :type x_test: 2D array\n",
    "    \n",
    "    :return: label predictions (0 or 1)\n",
    "    :rtype:  1D numpy array\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pred = sigmoid(x_test.dot(weight))\n",
    "    \n",
    "    f = lambda x : 0 if x <boundary else 1\n",
    "    \n",
    "    predictions = np.array([ f(x) for x in pred])\n",
    "    \n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_reg, losses_reg = reg_logistic_regression(y_train, x_train,lambda_,  initial_w, max_iters, gamma, all_losses=True) \n",
    "for i in range(1,8):\n",
    "    plt.figure(i)\n",
    "    plot_cost_max_iter(i, y_train,x_train,\"logistic_regression\",lambda_ = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0\n",
    "#w, loss = ridge_regression(y_train, x_train,lambda_=0.004)\n",
    "#print(loss)\n",
    "\n",
    "N,D = x_train.shape\n",
    "initial_w = np.zeros(D)\n",
    "max_iters = 10\n",
    "gamma = 1*1e-25\n",
    "\n",
    "w_reg, losses_reg = reg_logistic_regression(y_train, x_train,lambda_,  initial_w, max_iters, gamma, all_losses=True)\n",
    "\n",
    "print(np.diff(losses_reg) <= 0)\n",
    "print(np.round(losses_reg,7))\n",
    "\n",
    "w, losses = logistic_regression(y_train, x_train, initial_w, max_iters, gamma, all_losses=True)\n",
    "\n",
    "print(np.diff(losses) <= 0)\n",
    "print(np.round(losses,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T15:00:03.853432Z",
     "start_time": "2020-10-18T15:00:02.657630Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_ = 20\n",
    "#w, loss = ridge_regression(y_train, x_train,lambda_=0.004)\n",
    "#print(loss)\n",
    "\n",
    "N,D = x_train.shape\n",
    "initial_w = np.zeros(D)\n",
    "max_iters = 10\n",
    "gamma = 1*1e-25\n",
    "\n",
    "w_reg, losses_reg = reg_logistic_regression(y_train, x_train,lambda_,  initial_w, max_iters, gamma, all_losses=True)\n",
    "\n",
    "print(np.diff(losses_reg) <= 0)\n",
    "print(np.round(losses_reg,7))\n",
    "\n",
    "w, losses = logistic_regression(y_train, x_train, initial_w, max_iters, gamma, all_losses=True)\n",
    "\n",
    "print(np.diff(losses) <= 0)\n",
    "print(np.round(losses,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T09:42:57.687146Z",
     "start_time": "2020-10-17T09:42:57.684154Z"
    }
   },
   "source": [
    "##  Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T15:00:03.866396Z",
     "start_time": "2020-10-18T15:00:03.855427Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def K_Cross_Validation(x, y, K,gamma = 0):\n",
    "    #Initialization of all needed arrays\n",
    "    test_loss = np.zeros(K)\n",
    "    train_loss = np.zeros(K)\n",
    "    weights = np.zeros((K,x.shape[1]))\n",
    "    accuracy = np.zeros(K)\n",
    "    indices = build_k_indices(y, K)\n",
    "    max_iters = 10000\n",
    "    N,D = x.shape\n",
    "    initial_w = np.zeros(D)\n",
    "    for i in range(K):\n",
    "        test_indices = indices[i]\n",
    "        y_test = y[test_indices]\n",
    "        y_train = np.delete(y,test_indices)\n",
    "        x_test = x[test_indices,:]\n",
    "        x_train = np.delete(x,test_indices,axis=0)\n",
    "        ### ADAPT METHOD & LOSS\n",
    "        #weights[i], train_loss[i] = ridge_regression(y_train, x_train,_lambda)\n",
    "        weights[i], train_loss[i] = reg_batch_logistic_regression(y_train, x_train,1*1e-15, initial_w, max_iters, gamma,20 )\n",
    "        #test_loss[i] =  compute_ridge_loss(y_test,x_test,weights[i],_lambda)\n",
    "        test_loss[i] = loss_of_logistic_regression(x_test,y_test,weights[i])\n",
    "        \n",
    "        \n",
    "        #Calculate predictions of the model\n",
    "        predictions = predict(weights[i] , x_test)\n",
    "        #Calculate accuracy of the model\n",
    "        accuracy[i] = np.sum(predictions == y_test) / len(y_test)\n",
    "        \n",
    "    return accuracy, test_loss, train_loss, np.mean(weights, axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T15:00:10.616318Z",
     "start_time": "2020-10-18T15:00:03.868392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8126977777777779\n",
      "[24386.10443188 23848.95750244 23783.05866994 24342.3934888 ]\n",
      "[72195.99348944 71135.49482471 70572.25785411 71399.46206014]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#accs,test_loss, train_loss, w = K_Cross_Validation(x_train,y_train,4,0.001)\n",
    "accs,test_loss, train_loss, w = K_Cross_Validation(x_train,y_train,4, 0.001)\n",
    "plt.boxplot(accs)\n",
    "print(accs.mean())\n",
    "print(test_loss)\n",
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T15:01:50.713442Z",
     "start_time": "2020-10-18T15:00:10.617261Z"
    }
   },
   "outputs": [],
   "source": [
    "#def Tune_lambda(xt_training, y_training, K, gamma_range):\n",
    "def Tune_gamma(xt_training, y_training, K, gamma_range = 31):  \n",
    "    start_power = -40\n",
    "    stop_power = -10\n",
    "    gammas = np.logspace(start_power, stop_power, gamma_range)\n",
    "    max_acc = 0\n",
    "    min_loss = np.inf\n",
    "    #opt_lambda = 0\n",
    "    opt_gamma = 1e-40\n",
    "    accuracies = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for i, gamma in enumerate(gammas):\n",
    "        accuracy,test,train,w = K_Cross_Validation(xt_training, y_training, K, gamma)\n",
    "        accuracies.append([gamma,np.median(accuracy)])\n",
    "        train_losses.append([gamma,np.median(train)])\n",
    "        test_losses.append([gamma,np.median(test)])\n",
    "        if (np.median(test) < min_loss):\n",
    "            min_loss = np.median(test)\n",
    "            max_acc = np.median(accuracy)\n",
    "            opt_gamma = gamma\n",
    "                \n",
    "    return opt_gamma , max_acc, np.array(accuracies), np.array(train_losses), np.array(test_losses)\n",
    "opt_gamma, max_acc, acc ,train, test= Tune_gamma(x_train, y_train, 5,11)\n",
    "\n",
    "print(f\"optimal acc = {max_acc} with gamma= {opt_gamma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T15:01:51.375638Z",
     "start_time": "2020-10-18T15:01:50.714443Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(acc[:,0],acc[:,1], label=\"accuracy\")\n",
    "plt.plot(train[:,0],train[:,1],alpha=0.5,label=\"train error\")\n",
    "plt.plot(test[:,0],test[:,1],alpha=0.5, label = \"test eror\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"MSE/accuracy\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Logistic regression - Tuning the learning rate gamma (logarithmic scale)\")\n",
    "plt.savefig('Acuracy_logistic_regression.png',bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model is clearly overfitting, needs to be fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predictions on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T08:06:43.087742Z",
     "start_time": "2020-10-26T08:06:14.343034Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_w = np.zeros(x_train.shape[1])\n",
    "gamma = 1e-25\n",
    "max_iters = 100\n",
    "w_opt,loss = logistic_regression(y_train, x_train, initial_w, max_iters, gamma, all_losses=False)\n",
    "p = predict(w_opt,x_test)\n",
    "print((p==y_test).mean())\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
